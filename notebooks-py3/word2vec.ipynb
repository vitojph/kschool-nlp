{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de `word2vec` con `gensim`\n",
    "\n",
    "\n",
    "En la siguiente celda, importamos las librerías necesarias y configuramos los mensajes de los logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim, logging, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de un modelo\n",
    "\n",
    "Implemento una clase `Corpus` con un iterador sobre un directorio que contiene ficheros de texto. Utilizaré una instancia de `Corpus` para poder procesar de manera más eficiente una colección, sin necesidad de cargarlo previamente en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    '''Clase Corpus que permite leer de manera secuencial un directorio de documentos de texto'''\n",
    "    \n",
    "    def __init__(self, directorio):\n",
    "        self.directory = directorio\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fichero in os.listdir(self.directory):\n",
    "            for linea in open(os.path.join(self.directory, fichero)):\n",
    "                yield linea.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CORPUSDIR` contiene una colección de noticias en español (normalizada previamente a minúsculas y sin signos de puntuación) con alrededor de 150 millones de palabras. Entrenamos un modelo en un solo paso, ignorando aquellos tokens que aparecen menos de 10 veces, para descartar erratas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CORPUSDIR = 'PATH_TO_YOUR_CORPUS_DIRECTORY'\n",
    "oraciones = Corpus(CORPUSDIR)\n",
    "model = gensim.models.Word2Vec(oraciones, min_count=10, size=150, workers=2)\n",
    "\n",
    "# el modelo puede entrenarse en dos pasos sucesivos pero por separado\n",
    "#model = gensim.models.Word2Vec() # modelo vacío\n",
    "#model.build_vocab(oraciones)  # primera pasada para crear la lista de vocabulario\n",
    "#model.train(other_sentences)  # segunda pasada para calcula vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez completado el entrenamiento (después de casi 30 minutos), guardamos el modelo en disco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('PATH_TO_YOUR_MODEL.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el futuro, podremos utilizar este modelo cargándolo en memoria con la instrucción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = gensim.models.Word2Vec.load('PATH_TO_YOUR_MODEL.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando nuestro modelo\n",
    "\n",
    "El objeto `model` contiene una enorme matriz de números: una tabla, donde cada fila es uno de los términos del vocabulario reconocido y cada columna es una de las características que permiten modelar el significado de dicho término.\n",
    "\n",
    "En nuestro modelo, tal y como está entrenado, tenemos más de 26 millones de términos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada término del vocabulario está representado como un vector con 150 dimensiones: 105 características. Podemos acceder al vector de un término concreto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['azul'], '\\n')\n",
    "\n",
    "print(model['verde'], '\\n')\n",
    "\n",
    "print(model['microsoft'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Estos vectores no nos dicen mucho, salvo que contienen números muy pequeños :-/\n",
    "\n",
    "El mismo objeto `model` permite acceder a una serie de funcionalidades ya implementadas que nos van a permitir evaluar formal e informalmente el modelo. Por el momento, nos contentamos con los segundo: vamos a revisar visualmente los significados que nuestro modelo ha aprendido por su cuenta. \n",
    "\n",
    "Podemos calcular la similitud semántica entre dos términos usando el método `similarity`, que nos devuelve un número entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hombre - mujer', model.similarity('hombre', 'mujer'))\n",
    "\n",
    "print('madrid - parís', model.similarity('madrid', 'parís'))\n",
    "\n",
    "print('perro - gato', model.similarity('perro', 'gato'))\n",
    "\n",
    "print('gato - periódico', model.similarity('gato', 'periódico'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos seleccionar el término que no encaja a partir de una determinada lista de términos usando el método `doesnt_match`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista1 = 'madrid barcelona gonzález washington'.split()\n",
    "print('en la lista', ' '.join(lista1), 'sobra:', model.doesnt_match(lista1))\n",
    "\n",
    "lista2 = 'psoe pp ciu epi'.split()\n",
    "print('en la lista', ' '.join(lista2), 'sobra:', model.doesnt_match(lista2))\n",
    "\n",
    "lista3 = 'publicaron declararon soy negaron'.split()\n",
    "print('en la lista', ' '.join(lista3), 'sobra:', model.doesnt_match(lista3))\n",
    "\n",
    "lista3 = 'homero saturno cervantes shakespeare cela'.split()\n",
    "print('en la lista', ' '.join(lista3), 'sobra:', model.doesnt_match(lista3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos buscar los términos más similares usando el método `most_similar` de nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminos = 'psoe chicago sevilla aznar podemos estuvieron'.split()\n",
    "\n",
    "for t in terminos:\n",
    "    print(t, '==>', model.most_similar(t), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el mismo método `most_similar` podemos combinar vectores de palabras tratando de jugar con los rasgos semánticos de cada una de ellas para descubrir nuevas relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('==> alcalde + mujer - hombre')\n",
    "most_similar = model.most_similar(positive=['alcalde', 'mujer'], negative=['hombre'], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print('==> madrid + filipinas - españa')\n",
    "most_similar = model.most_similar(positive=['madrid', 'filipinas'], negative=['españa'], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
